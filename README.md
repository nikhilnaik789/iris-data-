# kNN From Scratch

## Introduction

This repository consists of code and example implementations for my medium article on building k-Nearest Neighbors from scratch and evaluating it using k-Fold Cross validation which is also built from scratch

For PyPI package version please refer to this repository

## k-Nearest Neighbors
k-Nearest Neighbors, kNN for short, is a very simple but powerful technique used for making predictions. The principle behind kNN is to use “most similar historical examples to the new data.”

## k-Nearest Neighbors in 4 easy steps
Choose a value for k
Find the distance of the new point to each record of training data
Get the k-Nearest Neighbors
Making Predictions
## k-Fold Cross Validation
This technique involves randomly dividing the dataset into k-groups or folds of approximately equal size. The first fold is kept for testing and the model is trained on remaining k-1 folds.

![grid_search_cross_validation](https://github.com/nikhilnaik789/iris-data-set/assets/141207725/15d763f4-808a-42db-b453-c1d7d05f62f7)

## Datasets Used
The datasets used here are taken from UCI Machine Learning Repository
## Applying LabelEncoder on entire dataframe
`from sklearn import preprocessing`

`df = pd.DataFrame(data)`

`df = df.apply(preprocessing.LabelEncoder().fit_transform)`

## References


